{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-27T23:42:41.698676Z",
     "start_time": "2017-10-27T23:42:41.695152Z"
    }
   },
   "source": [
    "This notebook is a little easier for beginners because it uses pytorch. You need to clone a repo to get it working:\n",
    "\n",
    "```sh\n",
    "# you need this repo, so clone it\n",
    "git clone https://github.com/wassname/DeepRL.git\n",
    "cd DeepRL\n",
    "git reset --hard aeae2c5d585e5853dc638968b1f090eb60abd351\n",
    "cd ..\n",
    "mkdir data log evaluation_log\n",
    "```\n",
    "\n",
    "This contains some minor modifications from https://github.com/ShangtongZhang/DeepRL.git\n",
    "\n",
    "The notebook tries DPPG with the [EIIE model](https://arxiv.org/pdf/1706.10059.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T03:51:12.766648Z",
     "start_time": "2018-02-18T03:51:12.763971Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also uncommented reward normalization in DDPG_agent.py#L64 because otherwise my small reward les to large Q's, inf losses, and NaN actions and weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T06:06:08.118663Z",
     "start_time": "2018-02-18T06:06:07.274323Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:__main__ logger started.\n"
     ]
    }
   ],
   "source": [
    "# plotting\n",
    "%matplotlib notebook\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# numeric\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import pandas as pd\n",
    "\n",
    "# utils\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from collections import Counter\n",
    "import tempfile\n",
    "import logging\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# logging\n",
    "logger = log = logging.getLogger(__name__)\n",
    "log.setLevel(logging.INFO)\n",
    "logging.basicConfig()\n",
    "log.info('%s logger started.', __name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T06:06:08.150314Z",
     "start_time": "2018-02-18T06:06:08.122827Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.sys.path.append(os.path.abspath('.'))\n",
    "os.sys.path.append(os.path.abspath('DeepRL'))\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T06:04:03.382623Z",
     "start_time": "2018-01-15T06:04:03.312027Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T06:06:08.172640Z",
     "start_time": "2018-02-18T06:06:08.152417Z"
    }
   },
   "outputs": [],
   "source": [
    "# params\n",
    "window_length = 50\n",
    "steps = 128\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T06:06:08.209252Z",
     "start_time": "2018-02-18T06:06:08.176664Z"
    }
   },
   "outputs": [],
   "source": [
    "# save dir\n",
    "import datetime\n",
    "ts = datetime.datetime.utcnow().strftime('%Y%m%d_%H-%M-%S')\n",
    "\n",
    "save_path = './outputs/pytorch-DDPG/pytorch-DDPG-EIIE-action-crypto-%s.model' % ts\n",
    "save_path\n",
    "try:\n",
    "    os.makedirs(os.path.dirname(save_path))\n",
    "except OSError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T06:06:08.984401Z",
     "start_time": "2018-02-18T06:06:08.212090Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorboard --logdir runs/ddpg-20180314_16-10-33\n"
     ]
    }
   ],
   "source": [
    "# setup tensorboard logging\n",
    "from tensorboard_logger import configure, log_value\n",
    "tag = 'ddpg-' + ts\n",
    "print('tensorboard --logdir '+\"runs/\" + tag)\n",
    "try:\n",
    "    configure(\"runs/\" + tag)\n",
    "except ValueError as e:\n",
    "    print(e)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T04:25:35.539014Z",
     "start_time": "2018-01-23T04:25:32.708434Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T06:06:09.100223Z",
     "start_time": "2018-02-18T06:06:08.986130Z"
    }
   },
   "outputs": [],
   "source": [
    "from rl_portfolio_management.environments.portfolio import PortfolioEnv\n",
    "from rl_portfolio_management.util import MDD, sharpe, softmax\n",
    "from rl_portfolio_management.wrappers import SoftmaxActions, TransposeHistory, ConcatStates\n",
    "\n",
    "df_train = pd.read_hdf('./data/poloniex_30m.hf',key='train')\n",
    "df_test = pd.read_hdf('./data/poloniex_30m.hf',key='test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T06:17:58.191717Z",
     "start_time": "2018-01-15T06:17:56.636432Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T06:06:09.175657Z",
     "start_time": "2018-02-18T06:06:09.112261Z"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "class DeepRLWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.render_on_reset = False\n",
    "        \n",
    "        self.state_dim = self.observation_space.shape\n",
    "        self.action_dim = self.action_space.shape[0]\n",
    "        \n",
    "        self.name = 'PortfolioEnv'\n",
    "        self.success_threshold = 2\n",
    "        \n",
    "    def normalize_state(self, state):\n",
    "        return state\n",
    "    \n",
    "    def step(self, action):\n",
    "        state, reward, done, info =self.env.step(action)\n",
    "        reward*=1e4 # often reward scaling is important sooo...\n",
    "        return state, reward, done, info\n",
    "    \n",
    "    def reset(self):        \n",
    "        # here's a roundabout way to get it to plot on reset\n",
    "        if self.render_on_reset: \n",
    "            self.env.render('notebook')\n",
    "\n",
    "        return self.env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T06:06:09.290527Z",
     "start_time": "2018-02-18T06:06:09.195655Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 51, 3), (4, 51, 3))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def task_fn():\n",
    "    env = PortfolioEnv(df=df_train, steps=steps, output_mode='EIIE')\n",
    "    env = TransposeHistory(env)\n",
    "    env = ConcatStates(env)\n",
    "    env = SoftmaxActions(env)\n",
    "    env = DeepRLWrapper(env)\n",
    "    return env\n",
    "\n",
    "def task_fn_test():\n",
    "    env = PortfolioEnv(df=df_test, steps=steps, output_mode='EIIE')\n",
    "    env = TransposeHistory(env)\n",
    "    env = ConcatStates(env)\n",
    "    env = SoftmaxActions(env)\n",
    "    env = DeepRLWrapper(env)\n",
    "    return env\n",
    "    \n",
    "# sanity check\n",
    "task = task_fn()\n",
    "task.reset().shape, task.step(task.action_space.sample())[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T06:06:09.450771Z",
     "start_time": "2018-02-18T06:06:09.292152Z"
    }
   },
   "outputs": [],
   "source": [
    "# load\n",
    "import pickle\n",
    "import shutil\n",
    "\n",
    "def save_ddpg(agent):\n",
    "    agent_type = agent.__class__.__name__\n",
    "    save_file = 'data/%s-%s-model-%s.bin' % (agent_type, config.tag, agent.task.name)\n",
    "    agent.save(save_file)\n",
    "    print(save_file)\n",
    "    \n",
    "\n",
    "def load_ddpg(agent):\n",
    "    agent_type = agent.__class__.__name__\n",
    "    save_file = 'data/%s-%s-model-%s.bin' % (agent_type, config.tag, agent.task.name)\n",
    "    new_states = pickle.load(open(save_file, 'rb'))\n",
    "    states = agent.worker_network.load_state_dict(new_states)\n",
    "\n",
    "\n",
    "def load_stats_ddpg(agent):\n",
    "    agent_type = agent.__class__.__name__\n",
    "    online_stats_file = 'data/%s-%s-online-stats-%s.bin' % (\n",
    "                    agent_type, config.tag, agent.task.name)\n",
    "    try:\n",
    "        steps, rewards = pickle.load(open(online_stats_file, 'rb'))\n",
    "    except FileNotFoundError:\n",
    "        steps =[]\n",
    "        rewards=[]\n",
    "    df_online = pd.DataFrame(np.array([steps, rewards]).T, columns=['steps','rewards'])\n",
    "    if len(df_online):\n",
    "        df_online['step'] = df_online['steps'].cumsum()\n",
    "        df_online.index.name = 'episodes'\n",
    "    \n",
    "    stats_file = 'data/%s-%s-all-stats-%s.bin' % (agent_type, config.tag, agent.task.name)\n",
    "    try:\n",
    "        stats = pickle.load(open(stats_file, 'rb'))\n",
    "    except FileNotFoundError:\n",
    "        stats = {}\n",
    "    df = pd.DataFrame(stats[\"test_rewards\"], columns=['rewards'])\n",
    "    if len(df):\n",
    "#         df[\"steps\"]=range(len(df))*50\n",
    "\n",
    "        df.index.name = 'episodes'\n",
    "    return df_online, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T06:02:01.252356Z",
     "start_time": "2018-01-15T06:02:01.208525Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T06:06:09.751094Z",
     "start_time": "2018-02-18T06:06:09.453781Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from agent import ProximalPolicyOptimization, DisjointActorCriticNet #, DeterministicActorNet, DeterministicCriticNet\n",
    "from component import GaussianPolicy, HighDimActionReplay, OrnsteinUhlenbeckProcess\n",
    "from utils import Config, Logger\n",
    "import gym\n",
    "import torch\n",
    "gym.logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T05:24:46.546070Z",
     "start_time": "2018-01-15T05:24:46.542443Z"
    }
   },
   "source": [
    "# Alg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T06:06:10.618760Z",
     "start_time": "2018-02-18T06:06:09.753415Z"
    }
   },
   "outputs": [],
   "source": [
    "# Modified from https://github.com/ShangtongZhang/DeepRL to log to tensorboard\n",
    "\n",
    "from utils.normalizer import Normalizer\n",
    "\n",
    "null_normaliser = lambda x:x\n",
    "\n",
    "class DDPGAgent:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.task = config.task_fn()\n",
    "        self.worker_network = config.network_fn()\n",
    "        self.target_network = config.network_fn()\n",
    "        self.target_network.load_state_dict(self.worker_network.state_dict())\n",
    "        self.actor_opt = config.actor_optimizer_fn(self.worker_network.actor.parameters())\n",
    "        self.critic_opt = config.critic_optimizer_fn(self.worker_network.critic.parameters())\n",
    "        self.replay = config.replay_fn()\n",
    "        self.random_process = config.random_process_fn()\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.total_steps = 0\n",
    "\n",
    "        self.state_normalizer = Normalizer(self.task.state_dim) # null_normaliser # \n",
    "        self.reward_normalizer = Normalizer(1)\n",
    "\n",
    "    def soft_update(self, target, src):\n",
    "        for target_param, param in zip(target.parameters(), src.parameters()):\n",
    "            target_param.data.copy_(target_param.data * (1.0 - self.config.target_network_mix) +\n",
    "                                    param.data * self.config.target_network_mix)\n",
    "\n",
    "    def save(self, file_name):\n",
    "        with open(file_name, 'wb') as f:\n",
    "            torch.save(self.worker_network.state_dict(), f)\n",
    "\n",
    "    def episode(self, deterministic=False, video_recorder=None):\n",
    "        self.random_process.reset_states()\n",
    "        state = self.task.reset()\n",
    "        state = self.state_normalizer(state)\n",
    "\n",
    "        config = self.config\n",
    "        actor = self.worker_network.actor\n",
    "        critic = self.worker_network.critic\n",
    "        target_actor = self.target_network.actor\n",
    "        target_critic = self.target_network.critic\n",
    "\n",
    "        steps = 0\n",
    "        total_reward = 0.0\n",
    "        while True:\n",
    "            actor.eval()\n",
    "            action = actor.predict(np.stack([state])).flatten()\n",
    "            if not deterministic:\n",
    "                action += self.random_process.sample()\n",
    "            next_state, reward, done, info = self.task.step(action)\n",
    "            if video_recorder is not None:\n",
    "                video_recorder.capture_frame()\n",
    "            done = (done or (config.max_episode_length and steps >= config.max_episode_length))\n",
    "            next_state = self.state_normalizer(next_state) * config.reward_scaling\n",
    "            total_reward += reward\n",
    "            \n",
    "            # tensorboard logging\n",
    "            prefix = 'test_' if deterministic else ''\n",
    "            log_value(prefix + 'reward', reward, self.total_steps)\n",
    "#             log_value(prefix + 'action', action, steps)\n",
    "            log_value('memory_size', self.replay.size(), self.total_steps)     \n",
    "            for key in info:\n",
    "                log_value(key, info[key], self.total_steps)     \n",
    "            \n",
    "            reward = self.reward_normalizer(reward)\n",
    "\n",
    "            if not deterministic:\n",
    "                self.replay.feed([state, action, reward, next_state, int(done)])\n",
    "                self.total_steps += 1\n",
    "\n",
    "            steps += 1\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            if not deterministic and self.replay.size() >= config.min_memory_size:\n",
    "                self.worker_network.train()\n",
    "                experiences = self.replay.sample()\n",
    "                states, actions, rewards, next_states, terminals = experiences\n",
    "                q_next = target_critic.predict(next_states, target_actor.predict(next_states))\n",
    "                terminals = critic.to_torch_variable(terminals).unsqueeze(1)\n",
    "                rewards = critic.to_torch_variable(rewards).unsqueeze(1)\n",
    "                q_next = config.discount * q_next * (1 - terminals)\n",
    "                q_next.add_(rewards)\n",
    "                q_next = q_next.detach()\n",
    "                q = critic.predict(states, actions)\n",
    "                critic_loss = self.criterion(q, q_next)\n",
    "\n",
    "                critic.zero_grad()\n",
    "                self.critic_opt.zero_grad()\n",
    "                critic_loss.backward()\n",
    "                if config.gradient_clip:\n",
    "                    grad_critic = nn.utils.clip_grad_norm(self.worker_network.parameters(), config.gradient_clip)\n",
    "                self.critic_opt.step()\n",
    "\n",
    "                actions = actor.predict(states, False)\n",
    "                var_actions = Variable(actions.data, requires_grad=True)\n",
    "                q = critic.predict(states, var_actions)\n",
    "                q.backward(torch.ones(q.size()))\n",
    "\n",
    "                actor.zero_grad()\n",
    "                self.actor_opt.zero_grad()\n",
    "                actions.backward(-var_actions.grad.data)\n",
    "                if config.gradient_clip:\n",
    "                    grad_actor = nn.utils.clip_grad_norm(self.worker_network.parameters(), config.gradient_clip)\n",
    "                self.actor_opt.step()\n",
    "                \n",
    "                # tensorboard logging\n",
    "                log_value('critic_loss', critic_loss.cpu().data.numpy().squeeze(), self.total_steps)\n",
    "                log_value('loss_action', -q.sum(), self.total_steps)\n",
    "                if config.gradient_clip:\n",
    "                    log_value('grad_critic', grad_critic, self.total_steps)\n",
    "                    log_value('grad_actor', grad_actor, self.total_steps)\n",
    "\n",
    "                self.soft_update(self.target_network, self.worker_network)\n",
    "\n",
    "        return total_reward, steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T06:33:40.472361Z",
     "start_time": "2018-01-15T06:33:40.450577Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T06:06:10.648063Z",
     "start_time": "2018-02-18T06:06:10.620822Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T06:06:10.685519Z",
     "start_time": "2018-02-18T06:06:10.649679Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 51, 3), 4)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task.state_dim, task.action_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T06:06:11.251457Z",
     "start_time": "2018-02-18T06:06:10.688106Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from network.base_network import BasicNet\n",
    "\n",
    "class DeterministicActorNet(nn.Module, BasicNet):\n",
    "    def __init__(self,\n",
    "                 state_dim,\n",
    "                 action_dim,\n",
    "                 action_gate,\n",
    "                 action_scale,\n",
    "                 gpu=False,\n",
    "                 batch_norm=False,\n",
    "                 non_linear=F.relu):\n",
    "        super(DeterministicActorNet, self).__init__()\n",
    "\n",
    "        stride_time = state_dim[1] - 1 - 2 #\n",
    "        features = task.state_dim[0]\n",
    "        h0 = 2\n",
    "        h1 = 30\n",
    "        self.conv1 = nn.Conv2d(features, h0, (3, 1))\n",
    "        self.conv2 = nn.Conv2d(h0, h1, (stride_time, 1), stride=(stride_time, 1))\n",
    "        self.conv3 = nn.Conv2d((h1+1), 1, (1, 1))\n",
    "\n",
    "        self.action_scale = action_scale\n",
    "        self.action_gate = action_gate\n",
    "        self.non_linear = non_linear\n",
    "\n",
    "        if batch_norm:\n",
    "            self.bn1 = nn.BatchNorm1d(h0)\n",
    "            self.bn2 = nn.BatchNorm1d(h1)\n",
    "\n",
    "        self.batch_norm = batch_norm\n",
    "        BasicNet.__init__(self, None, gpu, False)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.to_torch_variable(x)\n",
    "        \n",
    "        w0 = x[:,:1,:1,:] # weights from last step \n",
    "        x = x[:,:,1:,:]\n",
    "        \n",
    "        phi0 = self.non_linear(self.conv1(x))\n",
    "        if self.batch_norm:\n",
    "            phi0 = self.bn1(phi0)\n",
    "        phi1 = self.non_linear(self.conv2(phi0))\n",
    "        h = torch.cat([phi1,w0], 1)\n",
    "        if self.batch_norm:\n",
    "            h = self.bn2(h)\n",
    "        \n",
    "        action = self.conv3(h)\n",
    "        \n",
    "        # add cash_bias before we softmax\n",
    "        cash_bias_int = 0\n",
    "        cash_bias = self.to_torch_variable(torch.ones(action.size())[:,:,:,:1] * cash_bias_int)\n",
    "        action = torch.cat([cash_bias, action], -1)\n",
    "        \n",
    "        batch_size = action.size()[0]\n",
    "        action = action.view((batch_size,-1))\n",
    "        if self.action_gate:\n",
    "            action = self.action_scale * self.action_gate(action)\n",
    "        return action\n",
    "\n",
    "    def predict(self, x, to_numpy=True):\n",
    "        y = self.forward(x)\n",
    "        if to_numpy:\n",
    "            y = y.cpu().data.numpy()\n",
    "        return y\n",
    "\n",
    "class DeterministicCriticNet(nn.Module, BasicNet):\n",
    "    def __init__(self,\n",
    "                 state_dim,\n",
    "                 action_dim,\n",
    "                 gpu=False,\n",
    "                 batch_norm=False,\n",
    "                 non_linear=F.relu):\n",
    "        super(DeterministicCriticNet, self).__init__()\n",
    "        stride_time = state_dim[1] - 1 - 2 #\n",
    "        self.features = features = task.state_dim[0]\n",
    "        h0=2\n",
    "        h1=20\n",
    "        self.action = actions = action_dim -1\n",
    "        self.conv1 = nn.Conv2d(features, h0, (3, 1))\n",
    "        self.conv2 = nn.Conv2d(h0, h1, (stride_time, 1), stride=(stride_time, 1))\n",
    "        self.layer3 = nn.Linear((h1+2)*actions, 1)\n",
    "        self.non_linear = non_linear\n",
    "\n",
    "        if batch_norm:\n",
    "            self.bn1 = nn.BatchNorm1d(h0)\n",
    "            self.bn2 = nn.BatchNorm1d(h1)\n",
    "        self.batch_norm = batch_norm\n",
    "\n",
    "        BasicNet.__init__(self, None, gpu, False)\n",
    "\n",
    "\n",
    "    def forward(self, x, action):\n",
    "        x = self.to_torch_variable(x)\n",
    "        action = self.to_torch_variable(action)[:,None,None,:-1] # remove cash bias\n",
    "        \n",
    "        w0 = x[:,:1,:1,:] # weights from last step \n",
    "        x = x[:,:,1:,:]\n",
    "        \n",
    "        phi0 = self.non_linear(self.conv1(x))\n",
    "        if self.batch_norm:\n",
    "            phi0 = self.bn1(phi0)\n",
    "        phi1 = self.non_linear(self.conv2(phi0))\n",
    "        h = torch.cat([phi1,w0,action], 1)\n",
    "        if self.batch_norm:\n",
    "            h = self.bn2(h)\n",
    "        \n",
    "        batch_size = x.size()[0]\n",
    "        action = self.layer3(h.view((batch_size,-1)))\n",
    "        return action\n",
    "\n",
    "    def predict(self, x, action):\n",
    "        return self.forward(x, action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T03:09:10.244296Z",
     "start_time": "2018-02-18T03:09:10.218211Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-10T04:53:05.213318Z",
     "start_time": "2018-02-10T04:53:05.209185Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T06:06:11.430338Z",
     "start_time": "2018-02-18T06:06:11.253821Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.DDPGAgent at 0x7fb0de216a90>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Config()\n",
    "config.task_fn = task_fn\n",
    "task = config.task_fn()\n",
    "config.actor_network_fn = lambda: DeterministicActorNet(\n",
    "    task.state_dim, task.action_dim, action_gate=None, action_scale=1.0, non_linear=F.relu, batch_norm=False, gpu=False)\n",
    "config.critic_network_fn = lambda: DeterministicCriticNet(\n",
    "    task.state_dim, task.action_dim, non_linear=F.relu, batch_norm=False, gpu=False)\n",
    "config.network_fn = lambda: DisjointActorCriticNet(config.actor_network_fn, config.critic_network_fn)\n",
    "config.actor_optimizer_fn = lambda params: torch.optim.Adam(params, lr=4e-5)\n",
    "config.critic_optimizer_fn =\\\n",
    "    lambda params: torch.optim.Adam(params, lr=5e-4, weight_decay=0.001)\n",
    "config.replay_fn = lambda: HighDimActionReplay(memory_size=600, batch_size=64)\n",
    "config.random_process_fn = \\\n",
    "    lambda: OrnsteinUhlenbeckProcess(size=task.action_dim, theta=0.15, sigma=0.2, sigma_min=0.00002, n_steps_annealing=10000)\n",
    "config.discount = 0.0\n",
    "\n",
    "config.min_memory_size = 50\n",
    "config.target_network_mix = 0.001\n",
    "config.max_steps = 10000\n",
    "config.max_episode_length = 3000\n",
    "config.target_network_mix = 0.01\n",
    "config.noise_decay_interval = 100000\n",
    "config.gradient_clip = 20\n",
    "config.min_epsilon = 0.1\n",
    "\n",
    "# Many papers have found rewards scaling to be an important parameter. But while they focus on the scaling factor\n",
    "# I think they should focus on the end variance with a range of 200-400. e.g. https://arxiv.org/pdf/1709.06560.pdf\n",
    "# Hard to tell for sure without experiments to prove it\n",
    "config.reward_scaling = 1000\n",
    "\n",
    "config.test_interval = 10\n",
    "config.test_repetitions = 1\n",
    "config.save_interval = 40\n",
    "config.logger = Logger('./log', gym.logger)\n",
    "config.tag = tag\n",
    "agent = DDPGAgent(config)\n",
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T07:56:13.422024Z",
     "start_time": "2018-02-18T06:06:11.432034Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gym:episode 1, reward -27.355657, avg reward -27.355657, total steps 128, episode step 128\n",
      "[2018-03-14 16:10:38,090] episode 1, reward -27.355657, avg reward -27.355657, total steps 128, episode step 128\n",
      "INFO:gym:episode 2, reward -26.799783, avg reward -27.077720, total steps 256, episode step 128\n",
      "[2018-03-14 16:10:39,905] episode 2, reward -26.799783, avg reward -27.077720, total steps 256, episode step 128\n",
      "INFO:gym:episode 3, reward -10.701181, avg reward -21.618874, total steps 384, episode step 128\n",
      "[2018-03-14 16:10:41,706] episode 3, reward -10.701181, avg reward -21.618874, total steps 384, episode step 128\n",
      "INFO:gym:episode 4, reward -1.617895, avg reward -16.618629, total steps 512, episode step 128\n",
      "[2018-03-14 16:10:43,530] episode 4, reward -1.617895, avg reward -16.618629, total steps 512, episode step 128\n",
      "INFO:gym:episode 5, reward -0.753965, avg reward -13.445696, total steps 640, episode step 128\n",
      "[2018-03-14 16:10:45,437] episode 5, reward -0.753965, avg reward -13.445696, total steps 640, episode step 128\n",
      "INFO:gym:episode 6, reward -3.661682, avg reward -11.815027, total steps 768, episode step 128\n",
      "[2018-03-14 16:10:47,290] episode 6, reward -3.661682, avg reward -11.815027, total steps 768, episode step 128\n",
      "INFO:gym:episode 7, reward -3.467400, avg reward -10.622509, total steps 896, episode step 128\n",
      "[2018-03-14 16:10:48,991] episode 7, reward -3.467400, avg reward -10.622509, total steps 896, episode step 128\n",
      "INFO:gym:episode 8, reward -0.236697, avg reward -9.324283, total steps 1024, episode step 128\n",
      "[2018-03-14 16:10:50,650] episode 8, reward -0.236697, avg reward -9.324283, total steps 1024, episode step 128\n",
      "INFO:gym:episode 9, reward -5.378914, avg reward -8.885908, total steps 1152, episode step 128\n",
      "[2018-03-14 16:10:52,359] episode 9, reward -5.378914, avg reward -8.885908, total steps 1152, episode step 128\n",
      "INFO:gym:episode 10, reward -1.282984, avg reward -8.125616, total steps 1280, episode step 128\n",
      "[2018-03-14 16:10:54,044] episode 10, reward -1.282984, avg reward -8.125616, total steps 1280, episode step 128\n",
      "INFO:gym:Testing...\n",
      "[2018-03-14 16:10:54,055] Testing...\n",
      "INFO:gym:Avg reward 2.490495(0.000000)\n",
      "[2018-03-14 16:10:54,553] Avg reward 2.490495(0.000000)\n",
      "INFO:gym:episode 11, reward -6.791267, avg reward -8.004311, total steps 1408, episode step 128\n",
      "[2018-03-14 16:10:56,197] episode 11, reward -6.791267, avg reward -8.004311, total steps 1408, episode step 128\n",
      "INFO:gym:episode 12, reward -0.957244, avg reward -7.417056, total steps 1536, episode step 128\n",
      "[2018-03-14 16:10:57,841] episode 12, reward -0.957244, avg reward -7.417056, total steps 1536, episode step 128\n",
      "INFO:gym:episode 13, reward -0.505790, avg reward -6.885420, total steps 1664, episode step 128\n",
      "[2018-03-14 16:10:59,516] episode 13, reward -0.505790, avg reward -6.885420, total steps 1664, episode step 128\n",
      "INFO:gym:episode 14, reward -0.106210, avg reward -6.401191, total steps 1792, episode step 128\n",
      "[2018-03-14 16:11:02,163] episode 14, reward -0.106210, avg reward -6.401191, total steps 1792, episode step 128\n",
      "INFO:gym:episode 15, reward -2.304385, avg reward -6.128070, total steps 1920, episode step 128\n",
      "[2018-03-14 16:11:04,210] episode 15, reward -2.304385, avg reward -6.128070, total steps 1920, episode step 128\n",
      "INFO:gym:episode 16, reward -6.498697, avg reward -6.151234, total steps 2048, episode step 128\n",
      "[2018-03-14 16:11:06,019] episode 16, reward -6.498697, avg reward -6.151234, total steps 2048, episode step 128\n",
      "INFO:gym:episode 17, reward -1.307758, avg reward -5.866324, total steps 2176, episode step 128\n",
      "[2018-03-14 16:11:07,772] episode 17, reward -1.307758, avg reward -5.866324, total steps 2176, episode step 128\n",
      "INFO:gym:episode 18, reward -4.106525, avg reward -5.768557, total steps 2304, episode step 128\n",
      "[2018-03-14 16:11:09,649] episode 18, reward -4.106525, avg reward -5.768557, total steps 2304, episode step 128\n",
      "INFO:gym:episode 19, reward -7.587878, avg reward -5.864311, total steps 2432, episode step 128\n",
      "[2018-03-14 16:11:11,491] episode 19, reward -7.587878, avg reward -5.864311, total steps 2432, episode step 128\n",
      "INFO:gym:episode 20, reward -2.165253, avg reward -5.679358, total steps 2560, episode step 128\n",
      "[2018-03-14 16:11:13,217] episode 20, reward -2.165253, avg reward -5.679358, total steps 2560, episode step 128\n",
      "INFO:gym:Testing...\n",
      "[2018-03-14 16:11:13,231] Testing...\n",
      "INFO:gym:Avg reward -2.571162(0.000000)\n",
      "[2018-03-14 16:11:13,601] Avg reward -2.571162(0.000000)\n",
      "INFO:gym:episode 21, reward -1.074122, avg reward -5.460061, total steps 2688, episode step 128\n",
      "[2018-03-14 16:11:15,301] episode 21, reward -1.074122, avg reward -5.460061, total steps 2688, episode step 128\n",
      "INFO:gym:episode 22, reward -4.950745, avg reward -5.436911, total steps 2816, episode step 128\n",
      "[2018-03-14 16:11:16,967] episode 22, reward -4.950745, avg reward -5.436911, total steps 2816, episode step 128\n",
      "INFO:gym:episode 23, reward -8.357975, avg reward -5.563913, total steps 2944, episode step 128\n",
      "[2018-03-14 16:11:18,714] episode 23, reward -8.357975, avg reward -5.563913, total steps 2944, episode step 128\n",
      "INFO:gym:episode 24, reward 0.534191, avg reward -5.309826, total steps 3072, episode step 128\n",
      "[2018-03-14 16:11:20,393] episode 24, reward 0.534191, avg reward -5.309826, total steps 3072, episode step 128\n",
      "INFO:gym:episode 25, reward -3.968726, avg reward -5.256182, total steps 3200, episode step 128\n",
      "[2018-03-14 16:11:22,086] episode 25, reward -3.968726, avg reward -5.256182, total steps 3200, episode step 128\n",
      "INFO:gym:episode 26, reward -1.606157, avg reward -5.115796, total steps 3328, episode step 128\n",
      "[2018-03-14 16:11:23,782] episode 26, reward -1.606157, avg reward -5.115796, total steps 3328, episode step 128\n",
      "INFO:gym:episode 27, reward -1.305087, avg reward -4.974659, total steps 3456, episode step 128\n",
      "[2018-03-14 16:11:25,425] episode 27, reward -1.305087, avg reward -4.974659, total steps 3456, episode step 128\n",
      "INFO:gym:episode 28, reward -0.280021, avg reward -4.806993, total steps 3584, episode step 128\n",
      "[2018-03-14 16:11:27,086] episode 28, reward -0.280021, avg reward -4.806993, total steps 3584, episode step 128\n",
      "INFO:gym:episode 29, reward 1.377169, avg reward -4.593746, total steps 3712, episode step 128\n",
      "[2018-03-14 16:11:28,730] episode 29, reward 1.377169, avg reward -4.593746, total steps 3712, episode step 128\n",
      "INFO:gym:episode 30, reward 0.808700, avg reward -4.413665, total steps 3840, episode step 128\n",
      "[2018-03-14 16:11:30,534] episode 30, reward 0.808700, avg reward -4.413665, total steps 3840, episode step 128\n",
      "INFO:gym:Testing...\n",
      "[2018-03-14 16:11:30,547] Testing...\n",
      "INFO:gym:Avg reward 0.484278(0.000000)\n",
      "[2018-03-14 16:11:31,127] Avg reward 0.484278(0.000000)\n",
      "INFO:gym:episode 31, reward -1.230777, avg reward -4.310991, total steps 3968, episode step 128\n",
      "[2018-03-14 16:11:34,026] episode 31, reward -1.230777, avg reward -4.310991, total steps 3968, episode step 128\n",
      "INFO:gym:episode 32, reward -1.064715, avg reward -4.209545, total steps 4096, episode step 128\n",
      "[2018-03-14 16:11:36,750] episode 32, reward -1.064715, avg reward -4.209545, total steps 4096, episode step 128\n",
      "INFO:gym:episode 33, reward -1.154138, avg reward -4.116957, total steps 4224, episode step 128\n",
      "[2018-03-14 16:11:38,729] episode 33, reward -1.154138, avg reward -4.116957, total steps 4224, episode step 128\n",
      "INFO:gym:episode 34, reward -1.250727, avg reward -4.032656, total steps 4352, episode step 128\n",
      "[2018-03-14 16:11:40,460] episode 34, reward -1.250727, avg reward -4.032656, total steps 4352, episode step 128\n",
      "INFO:gym:episode 35, reward -0.347851, avg reward -3.927376, total steps 4480, episode step 128\n",
      "[2018-03-14 16:11:42,210] episode 35, reward -0.347851, avg reward -3.927376, total steps 4480, episode step 128\n",
      "INFO:gym:episode 36, reward -4.471951, avg reward -3.942503, total steps 4608, episode step 128\n",
      "[2018-03-14 16:11:43,876] episode 36, reward -4.471951, avg reward -3.942503, total steps 4608, episode step 128\n",
      "INFO:gym:episode 37, reward -1.455960, avg reward -3.875299, total steps 4736, episode step 128\n",
      "[2018-03-14 16:11:45,553] episode 37, reward -1.455960, avg reward -3.875299, total steps 4736, episode step 128\n",
      "INFO:gym:episode 38, reward -1.800847, avg reward -3.820708, total steps 4864, episode step 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-03-14 16:11:47,371] episode 38, reward -1.800847, avg reward -3.820708, total steps 4864, episode step 128\n",
      "INFO:gym:episode 39, reward -0.349991, avg reward -3.731715, total steps 4992, episode step 128\n",
      "[2018-03-14 16:11:49,167] episode 39, reward -0.349991, avg reward -3.731715, total steps 4992, episode step 128\n",
      "INFO:gym:episode 40, reward -1.477228, avg reward -3.675353, total steps 5120, episode step 128\n",
      "[2018-03-14 16:11:50,818] episode 40, reward -1.477228, avg reward -3.675353, total steps 5120, episode step 128\n",
      "INFO:gym:Testing...\n",
      "[2018-03-14 16:11:50,832] Testing...\n",
      "INFO:gym:Avg reward -0.102582(0.000000)\n",
      "[2018-03-14 16:11:51,210] Avg reward -0.102582(0.000000)\n",
      "INFO:gym:episode 41, reward -2.862761, avg reward -3.655534, total steps 5248, episode step 128\n",
      "[2018-03-14 16:11:52,889] episode 41, reward -2.862761, avg reward -3.655534, total steps 5248, episode step 128\n",
      "INFO:gym:episode 42, reward -4.606534, avg reward -3.678177, total steps 5376, episode step 128\n",
      "[2018-03-14 16:11:54,601] episode 42, reward -4.606534, avg reward -3.678177, total steps 5376, episode step 128\n",
      "INFO:gym:episode 43, reward -0.494180, avg reward -3.604130, total steps 5504, episode step 128\n",
      "[2018-03-14 16:11:56,233] episode 43, reward -0.494180, avg reward -3.604130, total steps 5504, episode step 128\n",
      "INFO:gym:episode 44, reward -0.229451, avg reward -3.527433, total steps 5632, episode step 128\n",
      "[2018-03-14 16:11:57,870] episode 44, reward -0.229451, avg reward -3.527433, total steps 5632, episode step 128\n",
      "INFO:gym:episode 45, reward -0.099567, avg reward -3.451258, total steps 5760, episode step 128\n",
      "[2018-03-14 16:11:59,500] episode 45, reward -0.099567, avg reward -3.451258, total steps 5760, episode step 128\n",
      "INFO:gym:episode 46, reward -0.625913, avg reward -3.389838, total steps 5888, episode step 128\n",
      "[2018-03-14 16:12:01,232] episode 46, reward -0.625913, avg reward -3.389838, total steps 5888, episode step 128\n",
      "INFO:gym:episode 47, reward -0.608328, avg reward -3.330656, total steps 6016, episode step 128\n",
      "[2018-03-14 16:12:02,869] episode 47, reward -0.608328, avg reward -3.330656, total steps 6016, episode step 128\n",
      "INFO:gym:episode 48, reward -0.663712, avg reward -3.275095, total steps 6144, episode step 128\n",
      "[2018-03-14 16:12:04,522] episode 48, reward -0.663712, avg reward -3.275095, total steps 6144, episode step 128\n",
      "INFO:gym:episode 49, reward -1.982362, avg reward -3.248713, total steps 6272, episode step 128\n",
      "[2018-03-14 16:12:06,162] episode 49, reward -1.982362, avg reward -3.248713, total steps 6272, episode step 128\n",
      "INFO:gym:episode 50, reward -0.618166, avg reward -3.196102, total steps 6400, episode step 128\n",
      "[2018-03-14 16:12:07,813] episode 50, reward -0.618166, avg reward -3.196102, total steps 6400, episode step 128\n",
      "INFO:gym:Testing...\n",
      "[2018-03-14 16:12:07,827] Testing...\n",
      "INFO:gym:Avg reward 2.057587(0.000000)\n",
      "[2018-03-14 16:12:08,189] Avg reward 2.057587(0.000000)\n",
      "INFO:gym:episode 51, reward -0.326071, avg reward -3.139827, total steps 6528, episode step 128\n",
      "[2018-03-14 16:12:09,801] episode 51, reward -0.326071, avg reward -3.139827, total steps 6528, episode step 128\n",
      "INFO:gym:episode 52, reward -0.653778, avg reward -3.092018, total steps 6656, episode step 128\n",
      "[2018-03-14 16:12:11,450] episode 52, reward -0.653778, avg reward -3.092018, total steps 6656, episode step 128\n"
     ]
    }
   ],
   "source": [
    "from main import run_episodes\n",
    "agent.task._plot = agent.task._plot2 = None\n",
    "try:    \n",
    "    run_episodes(agent)\n",
    "except KeyboardInterrupt as e:\n",
    "    save_ddpg(agent)\n",
    "    raise(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T07:56:16.462355Z",
     "start_time": "2018-02-18T07:56:16.094057Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot rewards\n",
    "plt.figure()\n",
    "df_online, df = load_stats_ddpg(agent)\n",
    "sns.regplot(x=\"step\", y=\"rewards\", data=df_online, order=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T07:56:23.350929Z",
     "start_time": "2018-02-18T07:56:23.316549Z"
    }
   },
   "outputs": [],
   "source": [
    "# monthly growth\n",
    "portfolio_return = (1+df_online.rewards[-100:].mean())\n",
    "\n",
    "returns = task.unwrapped.src.data[0,:,:1]\n",
    "market_return = (1+returns).mean()\n",
    "market_return, portfolio_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T03:29:04.356761Z",
     "start_time": "2018-02-18T03:29:04.327173Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-10-30T00:20:53.430Z"
    }
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T03:37:21.522568Z",
     "start_time": "2018-02-18T03:37:21.454751Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T07:56:24.491787Z",
     "start_time": "2018-02-18T07:56:24.390618Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_algo(env, algo, seed=0):\n",
    "    \"\"\"\n",
    "    Runs and algo from https://github.com/Marigold/universal-portfolios on env\n",
    "    \n",
    "    https://github.com/Marigold/universal-portfolios/commit/e8970a82427522ef11b1c3cbf681e18b5fe8169c\n",
    "    \"\"\"\n",
    "    env.seed(0)\n",
    "    np.random.seed(0)\n",
    "\n",
    "    state = env.reset()\n",
    "    for i in range(env.unwrapped.sim.steps):\n",
    "        \n",
    "        history= pd.DataFrame(state[0,:,:], columns=env.unwrapped.src.asset_names)\n",
    "        # MPT wants a cash column, and it should be first\n",
    "        history['CASH']=1\n",
    "        history=history[['CASH'] + env.unwrapped.src.asset_names]\n",
    "#         cols = list(history.columns)\n",
    "#         cols[0]='CASH'\n",
    "#         history.columns = cols\n",
    "        \n",
    "        x=history.iloc[-1]\n",
    "        \n",
    "        last_b = env.unwrapped.sim.w0#[1:]\n",
    "\n",
    "        algo.init_step(history)\n",
    "        # some don't want history\n",
    "        try:\n",
    "            action = algo.step(x, last_b, history)\n",
    "        except TypeError:\n",
    "            action = algo.step(x, last_b)\n",
    "        \n",
    "        # might by dataframe\n",
    "        action = getattr(action, 'value', action)\n",
    "        \n",
    "        # For upt\n",
    "        if isinstance(action, np.matrixlib.defmatrix.matrix):\n",
    "            action = np.array(action.tolist()).T[0]\n",
    "            \n",
    "        \n",
    "\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        if done:\n",
    "            break   \n",
    "    df = pd.DataFrame(env.unwrapped.infos)\n",
    "    df.index = pd.to_datetime(df['date']*1e9)\n",
    "    return df['portfolio_value'], df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T07:56:25.669367Z",
     "start_time": "2018-02-18T07:56:24.530860Z"
    }
   },
   "outputs": [],
   "source": [
    "# use test env\n",
    "df_test = pd.read_hdf('./data/poloniex_30m.hf',key='test')\n",
    "test_steps=5000\n",
    "env_test = task_fn_test()\n",
    "agent.task = env_test\n",
    "agent.config.max_episode_length = test_steps\n",
    "agent.task.reset()\n",
    "np.random.seed(0)\n",
    "\n",
    "# run in deterministic mode, no training, no exploration\n",
    "agent.episode(True)\n",
    "agent.task.render('notebook')\n",
    "agent.task.render('notebook', True)\n",
    "\n",
    "df = pd.DataFrame(agent.task.unwrapped.infos)\n",
    "df.index = pd.to_datetime(df['date']*1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T04:01:49.504199Z",
     "start_time": "2018-02-18T04:01:49.467708Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T03:34:12.858016Z",
     "start_time": "2018-02-18T03:34:12.772885Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T07:56:29.025682Z",
     "start_time": "2018-02-18T07:56:25.704036Z"
    }
   },
   "outputs": [],
   "source": [
    "from universal import algos\n",
    "env = task.unwrapped\n",
    "price_cols = [col for col in df.columns if col.startswith('price')]\n",
    "for col in price_cols:\n",
    "    df[col]=df[col].cumprod()\n",
    "\n",
    "df = df[price_cols + ['portfolio_value']]\n",
    "    \n",
    "algo_dict=dict(\n",
    "    # Pick the same is in https://arxiv.org/pdf/1706.10059.pdf\n",
    "    # Benchmarks\n",
    "#     UCRP=algos.UP(),\n",
    "    \n",
    "    # Follow the winner\n",
    "    BestSoFar=algos.BestSoFar(cov_window=env_test.unwrapped.src.window_length-1),\n",
    "#     UniversalPortfolio=algos.UP(eval_points=1000),\n",
    "    ONS=algos.ONS(),\n",
    "    \n",
    "    # Follow the loser\n",
    "#     OnlineMovingAverageReversion=algos.OLMAR(window=env.src.window_length-1, eps=10), \n",
    "    RMR=algos.RMR(window=env_test.unwrapped.src.window_length-1, eps=10),\n",
    "#     PassiveAggressiveMeanReversion=algos.PAMR(),\n",
    "    \n",
    "    # Pattern matching\n",
    "    #     CorrelationDrivenNonparametricLearning=algos.CORN(window=30),\n",
    ")\n",
    "for name, algo in algo_dict.items():\n",
    "    print(name)\n",
    "    perf, _ = test_algo(env_test, algo)\n",
    "    perf.index=df.index\n",
    "    df[name]=perf\n",
    "\n",
    "# put portfolio value at end so we plot it on top and can therefore see it\n",
    "cols = list(df.columns.drop('portfolio_value'))+['portfolio_value']\n",
    "df=df[cols]\n",
    "\n",
    "\n",
    "df.plot(alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T03:53:15.784969Z",
     "start_time": "2018-02-18T03:53:15.660001Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "70px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
